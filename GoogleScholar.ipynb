{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-8-4b0910b19aeb>, line 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-4b0910b19aeb>\"\u001b[1;36m, line \u001b[1;32m52\u001b[0m\n\u001b[1;33m    HEADERS['User-Agent']=\"Mozilla/5.0\"\u001b[0m\n\u001b[1;37m                                       ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "'''\t\tCREATED BY GAURAV MITRA\t\t'''\n",
    "'''\t\tCRAWLS AND SCRAPES GOOGLE SCHOLAR WEBSITE ON ENTERING A QUERY\t\t'''\n",
    "# WARNINGS :  \n",
    "# \t\t1)\t\tGOOGLE SCHOLAR robots.txt SITE CLEARLY FORBIDS CRAWLERS AND SCRAPERS.\n",
    "#\t\t2)\t\tMOST OF THE DATA IN GOOGLE SCHOLAR IS INVISIBLE TO ROBOTS AND CRAWLERS\n",
    "#\t\t3)\t\tHENCE TOO MUCH QUERYING MAY LEAD TO TEMPORARY BAN ON YOUR IP ADDRESS\n",
    "\n",
    "'''\t\tLIGHTWEIGHT CRAWLER THAT CRAWLS THROUGH THE GOOGLE SCHOLAR SITE MOSTLY RESPECTING GOOGLE TERMS OF USE BUT MAY LEAD TO \n",
    "\t\tOCCASSIONAL VIOLATION OF THE robots.txt OF GOOGLE SCHOLAR   '''\n",
    "\n",
    "try :\n",
    "\timport urllib\n",
    "\timport urllib2\n",
    "\t#import BeautifulSoup as bs\n",
    "\timport re\n",
    "\timport random\n",
    "\timport robotparser as rp\n",
    "\timport htmlentitydefs\n",
    "\timport hashlib\n",
    "\timport sys\n",
    "\timport os\n",
    "\timport datetime\n",
    "\timport time\n",
    "except ImportError:\n",
    "\tprint 'Error : {}'.format(\"All Required Modules are Not Present\")\n",
    "\n",
    "HEADERS=dict()\n",
    "\n",
    "def initialization() :\n",
    "        return \"http://scholar.google.com\",\"/scholar?q=\"\n",
    "\n",
    "def parse_Robot_File():\n",
    "\trobot=rp.RobotFileParser()\n",
    "\tGOOGLE_SCHOLAR_MAIN_URL,QUERY_APPEND=initialization()\n",
    "\trobot.set_url(GOOGLE_SCHOLAR_MAIN_URL+\"/robots.txt\")\n",
    "\treturn robot.read()\n",
    "\t\n",
    "def random_number_seedset() :\n",
    "\tdate=datetime.datetime.now()\n",
    "\ttime_since_epoch=time.mktime(date.timetuple()) + date.microsecond/1000000.0\n",
    "\tmilliseond=time_since_epoch*1000\n",
    "\trandom.seed(milliseond)\n",
    "\n",
    "def create_fake_id() :\n",
    "\trandom_number_seedset()\n",
    "\trandom_string=str(random.random())\n",
    "\trandom_string=random_string.encode('utf8')\n",
    "\tfake_google_id=hashlib.md5(random_string)\n",
    "\tfake_google_id=fake_google_id.hexdigest()\n",
    "\tfake_google_id=fake_google_id[:16]\n",
    "\tHEADERS['Cookie']=\"GSP=ID={}:CF=4\".format(fake_google_id)\n",
    "    HEADERS['User-Agent']=\"Mozilla/5.0\"\n",
    "\n",
    "\n",
    "def Google_Scholar_Query(searchQuery):\n",
    "        create_fake_id(),\n",
    "        GOOGLE_SCHOLAR_URL,QUERY_APPEND=initialization()\n",
    "        searchQuery.replace(\" \",\"+\")\n",
    "        url = GOOGLE_SCHOLAR_URL + QUERY_APPEND + searchQuery\n",
    "        html=\"\"\n",
    "        try :\n",
    "                request = urllib2.Request(url, headers=HEADERS)\n",
    "                response = urllib2.urlopen(request)\n",
    "                page_data = response.read()\n",
    "                page_data = page_data.decode('utf8')\n",
    "                html=page_data\n",
    "        except urllib2.HTTPError : \n",
    "                print HEADERS\n",
    "                print '{0} : {1}'.format(\"HTTP 503 SERVICE UNAVAILABLE ERROR\",\"CHANGE USER-AGENT IN HEADERS TO FIX THE ERROR\")\n",
    "\n",
    "        link_parse = re.compile(r'<a href=\"(/scholar\\.bib\\?[^\"]*)')\n",
    "        links = link_parse.findall(html)\n",
    "        links = [re.sub('&(%s);' % '|'.join(htmlentitydefs.name2codepoint), lambda m: chr(htmlentitydefs.name2codepoint[m.group(1)]), item) for item in links]\n",
    "\n",
    "        output = list()\n",
    "        for link in links:\n",
    "                if not allowed_By_Robots(link) :\n",
    "                        continue\n",
    "                url = GOOGLE_SCHOLAR_URL+link\n",
    "                try :\n",
    "                        request = urllib2.Request(url, headers=header)\n",
    "                        response = urllib2.urlopen(request)\n",
    "                        bibtex = response.read()\n",
    "                        bibtex = bibtex.decode('utf8')\n",
    "                        output.append(bibtex)\n",
    "                except urllib2.HTTPError :\n",
    "                        print '{0} : {1}'.format(\"HTTP 503 SERVICE UNAVAILABLE ERROR\",\"CHANGE USER-AGENT IN HEADERS TO FIX THE ERROR\")\n",
    "                        continue\n",
    "        return output\n",
    "\n",
    "def allowed_By_Robots(link):\n",
    "\trobot=parse_Robot_File()\n",
    "\tif (robot.can_fetch(\"*\",link) is True) :\n",
    "\t\treturn True\n",
    "\telse :\n",
    "\t\treturn False\n",
    "\n",
    "if __name__==\"__main__\" :\n",
    "\tprint '{}'.format(\"You Have to Enter the Search Query and This Aplication will return a list of Citations that match your Query\")\n",
    "\tquery=raw_input(\"Enter the Name of the Paper or Query String : \")\n",
    "\tcitations_list=Google_Scholar_Query(query)\n",
    "\tprint citations_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
